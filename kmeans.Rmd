---
title: "kmeans"
output: html_document
---
```{r, setup, include=FALSE}
knitr::opts_chunk$set(
  eval=FALSE, warning=FALSE, error=FALSE, message=FALSE
)
```


This is an [R Markdown](https://rmarkdown.rstudio.com/) document. Follow the link to learn more about R Markdown and the notebook format used during the workshop.

# Setup

```{r, eval=TRUE, message=F}
library(readr) #reads rectangular csv data 
library(tidyverse) #data manipulation 
library(factoextra) #visualization
library(vip) #visualization 
library(reshape2) #restructure data, melt 
library(rattle) #wine data
```

# What is [Clustering](https://realpython.com/k-means-clustering-python/)

**Clustering** 

In cluster analysis we find the "natural groupings" of items based on one or more measurements. We have no dependent variable. Clustering is a type of unsupervised machine learning. The algorithm finds patterns in untagged data. Ultimately we are looking for clusters that are homogeneous within and heterogeneous across. Clusters can help us with graphical or verbal descriptions of our data, and to some degree purposes of prediction or prognoses (e.g., customer segmentation, symptom profiles).   

**K-means**

The k-means algorithm is one clustering method that separates data into k groups of equal variance and minimizes within-cluster sum-of-squares. The method uses numerical variables only, favors spherical clusters, scales well, and is commonly used. 

**Today**

  + Variable selection 
  + Preprocessing
  + Picking the number of clusters
  + Bonus: Profiling clusters 

# Data

We will be using two data sets throughout this workshop 

  + [Cereal data](https://www.kaggle.com/crawford/80-cereals) to demonstrate concepts 
  + [Wine data](https://cran.r-project.org/web/packages/rattle.data/rattle.data.pdf) to practice concepts 

```{r}
# cereal data
cereal <- read_csv("Cereals.csv")
```

What's in the data: 

```{r}
cereal
wine
```

## Variables 

k-means clustering requires complete cases

```{r}
#find cases with missing values 
cereal[!complete.cases(cereal),]
```
For this demonstration we will use:

  + calories: calories per serving
  + protein: grams of protein
  + fat: grams of fat
  + fiber: grams of dietary fiber 

We exclude non-numerical variables and those with missing data 

### Exercise

Does the wine data contain missing values? 

```{r}
```

## Variable Selection and Standardize 

We are working with incommensurate units. This requires us to first standardize variables. Variables are often scaled to have mean 0 and variance 1. 

```{r}
str(cereal)
scores <- cereal %>% 
          select(4:6,8) %>% #keeps only variables mentioned
          scale() %>% #scales the columns of a numeric matrix 
          as.data.frame() #coerces object to a data frame 
```

### Exercise 

Make sure the wine data is in the data frame format. Select only the columns for alcohol, malic, ash, alcalinity, magnesium, and phenols. Determine if scaling is appropriate.  

```{r}
```

# kmeans

Now that we've finished data pre-processing, we can find various cluster solutions. First we select a ballpark number of clusters (e.g. 2-6). 

```{r}
k2 <- kmeans(scores, centers = 2, nstart = 25)
str(k2)

k1 <- kmeans(scores, centers = 1, nstart = 25)
k3 <- kmeans(scores, centers = 3, nstart = 25)
k4 <- kmeans(scores, centers = 4, nstart = 25)
k5 <- kmeans(scores, centers = 5, nstart = 25)
k6 <- kmeans(scores, centers = 6, nstart = 25)
```

```{r, dpi=600}
#using the factoextra package, plots to compare
p1 <- fviz_cluster(k1, geom = "point", data = scores) + ggtitle("k = 1")
p2 <- fviz_cluster(k2, geom = "point", data = scores) + ggtitle("k = 2")
p3 <- fviz_cluster(k3, geom = "point",  data = scores) + ggtitle("k = 3")
p4 <- fviz_cluster(k4, geom = "point",  data = scores) + ggtitle("k = 4")
p5 <- fviz_cluster(k5, geom = "point",  data = scores) + ggtitle("k = 5")
p6 <- fviz_cluster(k6, geom = "point",  data = scores) + ggtitle("k = 6")


grid.arrange(p1, p2, p3, p4, p5, p6, nrow = 2)
```

# Number of Clusters

We can use summary statistics and domain experts to inform our selection of the optimal number of clusters. 

  + the silhouette statistic helps us pick the cluster solution with the largest overall width. To learn more about [silhouettes](https://www.sciencedirect.com/science/article/pii/0377042787901257?via%3Dihub). Essentially, this measure quantifies how much better the assigned cluster is to the next best one. The value ranges from 0 to 1. 
  + elbow method heuristics help us identify the bend in the WSS plot 
  + domain experts have a sense of how these clusters can be used 

```{r, dpi=600}
set.seed(404)
#function to compute total within-cluster sum of square 
wss <- function(k) {
  kmeans(scores, k, nstart = 10 )$tot.withinss
}

#Compute and plot wss for k = 1 to k = 15
k.values <- 1:15

#extract wss for 2-15 clusters
wss_values <- map_dbl(k.values, wss)

plot(k.values, wss_values,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")
```
```{r, dpi=600}
#using the factoextra package
set.seed(404)
fviz_nbclust(scores, kmeans, method = "wss")
fviz_nbclust(scores, kmeans, method = "silhouette")
fviz_nbclust(scores, kmeans, method = "silhouette")$data #silhouette widths 
```

# Optimal Cluster Solution
```{r}
set.seed(505)  #assures that you get same starting values
final <- kmeans(scores, 3, nstart = 25)
print(final)

c <- fviz_cluster(final, data = scores)
c

#cluster centers 
scores %>%
  mutate(Cluster = final$cluster) %>%
  group_by(Cluster) %>%
  summarise_all("mean")
```


# Bonus: Data Visulation with Boxplots 

```{r, fig.width=6, fig.height=6}
scores$cluster <- final$cluster
plotdf.m = melt(scores[,c(1:5)], id.var="cluster")  #transform data for plot
plotdf.m = plotdf.m[order(plotdf.m$variable, plotdf.m$cluster),]
```

```{r, fig.width=6, fig.height=6, dpi=600}
ggplot(data=plotdf.m, aes(x=as.factor(cluster), y=value)) + 
  geom_boxplot(aes(fill=as.factor(variable))) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black")) +
  theme(legend.position="top") +
  labs(y="Scores", x="Cluster") + 
  labs(fill=NULL) 
```

# Clustered Cereals 

```{r}
cereal$cluster <- scores$cluster 
#cluster 1
cereal %>% 
  filter(cluster==1) %>% #finds rows/cases where conditions are true
  head(5) #displays first rows 
#cluster 2
cereal %>% 
  filter(cluster==2) %>% #finds rows/cases where conditions are true
  head(3) #displays first rows 
#cluster 3
cereal %>% 
  filter(cluster==3) %>% #finds rows/cases where conditions are true
  head(5) #displays first rows 
```

### Exercise 

Describe clusters in the wine data 

# Learning More 

  + Check out our [blog](https://sites.northwestern.edu/researchcomputing/posts/) for an upcoming post containing online learning resources about clustering. 
  + If you have a question about clustering your data, request a free consultation with our data science consultants [here](https://app.smartsheet.com/b/form/2f2ec327e6164f83b588b7bbe2e2b56f) 

